# 1                再论弱律与概率收敛

在最后几类中，我们研究了弱大数定律和中心极限定理弱大数定律如下：

定理21.1（弱大数定律）。假设X1，X2，。。。是独立且相同分布的随机变量假设E | Xi |<∞使得EXi定义良好设EXi=μ。那么

如n·····。

回想一下，从上节课开始，P的定义如下：当n············下面的结果给出了一个直观的关于概率收敛的简单事实然而，这个结果有点难以证明（欢迎您尝试证明这个；结果本身对我们有用，但不是证明）。

引理21.2。如果X1，X2，。。。和Y1，Y2对于两个常数c和d，两个随机变量序列满足Xn→pc和Yn→pc

*1.*    Xn+Yn→P c+d

第页

*2.*    Xn-Yn→c-d

*3.*    XnYn→P cd

*4.*    Xn/Yn→P c/d，前提是d 6=0。

现在让我们回到弱大数定律注意（21.1）适用于随机变量X1，X2（只有独立的假设和相同的分布和期望的存在就足够了）。在随机变量具有有限方差的附加假设下，弱定律易于证明。我们在上节课中已经看到的这个证明，是基于切比雪夫不等式的

（二十九）

因为

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.gif)

如n·····结果，从（29）开始，我们得到（29）的左手边收敛到0，这意味着X′n→Pμ。

一般来说，如果Y1，Y2，。。。是一系列随机变量，其中EYn收敛到某个参数θ，var（Yn）收敛到零，然后Yn→Pθ。这在下面的结果中给出。

引理21.3假设Y1，Y2，。。。是一系列随机变量

*1.*    EYn→θas n→∞

*2.*    变量（Yn）→0作为n→∞。

第页

然后Yn→θ为n·····。

证明写Yn=EYn+（Yn-EYn）。切比雪夫不等式（以及var（Yn）·····）给出

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.gif)

每大于0，则Yn-EYn→P 0。这个和EYn→θ意味着（通过引理21.2的第一个断言）

第页

Yn=EYn+（Yn−EYn）→θ。

在数理统计中，当Yn→Pθ时，我们说Yn是θ的一致估计，或者简单地说Yn是θ的一致估计弱大数定律简单地说X′n与E（X1）是一致的更一般地，引理21.3指出，当E（Yn）>0和var（Yn）>0时，Yn与θ是一致的下面的例子展示了另外两种保持一致性的情况。

例21.4假设X1，X2，。。。i.i.d在（0，θ）上的均匀分布，对于固定θ>0。

然后，最大阶统计量x（n）＝max（x1，…，Xn）是一个一致的估计，即x，（n）~p p。我们可以从两个方面来看待这个问题第一种方法是利用上述结果（引理21.3）计算X（n）的均值和方差X（n）/θ是来自Unif（0,1）中n大小的i.i.d样本的最大阶统计量，正如我们在上一类中看到的，X（n）/θ具有β（n，1）分布。因此，使用Beta分布的均值和方差公式（这些公式见wikipedia），我们有

和

.

它给予

和

.

显然，EX（n）收敛到θ，var（X（n））分别收敛到0，这意味着（通过引理21.3）X（n）收敛到θ的概率。

有第二种（更直接的）方法可以看到X（n）→Pθ这包括写作

为所有人

当n··································根据概率收敛的定义，这表明X（n）>Pθ。

例21.5。假设X1，X2，。。。是具有平均μ和有限方差σ2的i.i.d观测值那么

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image006.gif)

是σ2的一致估计量。看到第一个音符

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image008.gif)

利用弱大数定律，以n····················。这是因为，对于i=1，…，n，σ-n2是i.i.d随机变量Yi=（Xi-μ）2的平均值。因此，弱定律表明，σ-n2在概率上收敛于EY1=E（X1-μ）2=σ2。

现在，为了证明这一点，我们的想法仅仅是将σˆn2与σ倬n2联系起来。具体操作如下：

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.gif)

上面右边的第一项通过弱大数定律收敛到σ2（注意，σ2=E（X1–）2）。第二项收敛到零是因为（引理21.2）。我们用引理

21.2再次得出σˆn2→Pσ2的结论注意，我们没有假设任何关于X1，X2，…，Xn的分布假设（唯一的要求是它们有均值零和方差σ2）。

顺便说一下，我们还可以通过

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image012.gif)

系数为1/（n−1）而不是1/n。这也会在概率上收敛到σ2，因为

.

由于上面的第一项在概率上收敛于σ2，而第二项在概率上收敛于σ1，因此乘积在概率上收敛于σ2（引理21.2）。

# 2                Slutsky定理、连续映射定理及其应用

我们还研究了中心极限定理的陈述和证明，如下所示。

定理22.1（中心极限定理）。假设Xi，i=1,2i.i.d，E（Xi）=μ，var（Xi）=σ2<∞那么

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image014.gif)

式中，X∏n=（X1+····+Xn）/n。

这里，分布的收敛性（→L）定义如下：序列Y1，Y2，。。。当P{Yn≤y}收敛于F（y）的连续点的每一个y的F（y）时，称为随机变量的分布收敛于F。虽然分布收敛是用cdfs定义的，但由于独立随机变量和的cdfs不易处理，故用矩母函数证明了CLT。

从统计学角度来看，CLT的一个重要结果是，它给出了μ的渐近有效置信区间事实上，由于CLT，我们

)作为n→∞

每a≤b，这与

)如n·····。

现在假设zα/2>0是实线上的点，使得Φ（zα/2）=1-α/2表示0<α<1然后取a=-zα/2和b=zα/2，我们推断

如n·····。

这意味着

（三十）

是μ的渐近100（1α）%置信区间（假设σ已知）。CLT的应用确保了对X1，X2是此结果所必需的。

区间（30）的问题在于它取决于在统计设置中未知的σ（唯一可用的数据是X1，…，Xn）自然的想法是用自然估计代替σ，如例21.5中定义的ˆσn：

（31个）

这将导致间隔：

（32）Slutsky定理下一步将暗示

1）（33）

这意味着（32）也是μ的渐近100（1α）%置信区间。

定理22.2（Slutsky定理）和Bn→pB，然后

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image016.gif)

.

我们经常使用的另一个有用的结果是连续映射定理：

定理22.3（连续映射定理）一。假设f是在Y值范围内连续的函数，那么f（Yn）→lf（Y）。

2。假设f在c处是连续的，那么f（Yn）→pf（c）。

这两个结果的一个直接应用是（33），如下所示。

例22.4。设X1，…，Xn为i.i.d观测值，平均值μ，方差σ2。我们需要考虑以下限制性分布：

（三十四）

式中，σˆn如（31）中所定义。注意

.

上面右手边的第一项通过通常的CLT在概率上收敛到N（0,1）对于第二项，请注意σn2→Pσ2（如例21.5所证明），因此应用连续映射定理

f（x）=pσ2/x意味着。这使得上面的第二项收敛于1的概率因此，我们可以用Slutsky定理来观察，由于上面的第一项在分布上收敛到N（0,1），而第二项在概率上收敛到1，所以随机变量Tn在分布上收敛到N（0,1）。因此，

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image018.gif)

仍然是100（1-α）%的渐近有效C.I（μ）。注意，我们还没有对X1，…，Xn假设任何分布假设特别是，数据可以是非高斯的。

（34）中的随机变量Tn称为样本t统计量。这个名字来源于t分布或t密度对于给定的整数k≥1，具有k个自由度的t密度是

变量

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image020.gif)

其中Z∼N（0,1），A具有k度的卡方密度（即A∼x2k），Z和A是独立随机变量。

现在，当X1，…，Xn是i.i.d N（μ，σ2）时，可以显示（稍后我们将看到如何这样做）

和

而且这两个随机变量是独立的因此，当X1，…，Xn为i.i.d n（μ，σ2）时，t统计量Tn具有n−1自由度的tdi分布。

因此

*1.*    当X1，…，Xn为i.i.d N（μ，σ2）时，样本t统计量Tn具有N-1自由度的t分布。

*2.*    当X1，…，Xn为i.i.d，平均μ和有限方差σ2（无分布假设）时，t统计量，t N在分布上收敛到N（0,1）。

有必要注意到，k自由度的t分布本身在分布上收敛到N（0,1）作为k→∞。

例22.5（伯努利参数估计）假设X1，X2，…，Xn是具有Ber（p）分布的i.i.d然后CLT给出

它给予

如n·····。

这不会直接导致p的C.I。要做到这一点，用X'代替分母中的p是很自然的。这样做是因为

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image022.gif)

根据Slutsky定理，上述随机变量在分布上收敛到N（0,1）为了给出更多的细节，我们使用了这样一个事实：上面的第一个随机变量通过CLT在分布上收敛到N（0,1），第二个随机变量在概率上收敛到1（基本上然后使用连续映射定理）。这让我们可以推断

如n·····。

以便

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image024.gif)

是p的渐近有效100（1α）%C.I。

例22.6（泊松平均估计）假设X1，X2，…，Xn是具有Poi（λ）分布的i.i.d。然后CLT给出

它给予

如n·····。

很难将其转换为λ的C.I如果我们能用X′代替分母中的λ，这将变得简单得多。这样做是因为

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image026.gif)

根据Slutsky定理，上述随机变量在分布上收敛到N（0,1）（这里我们使用的是弱大数定律的结果）这让我们可以推断

如n·····。

以便

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image028.gif)

是λ的渐近有效100（1-α）%C.I。

例22.7（样本方差的渐近分布）。设X1，X2为i.i.d，平均值μ，有限方差σ2让

.

我们知道σˆn2→Pσ2。我们也能找到极限分布吗？要做到这一点，写下

.

现在到了中央电视台，

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image030.gif)

式中，τ2=var（（X1～）2）（当然，我们假设τ2＜∞），根据Slutsky定理，

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image032.gif)

因此，通过Slutsky定理，我们得到

.

Slutsky定理的另一个简单结果是：。

L√P

事实：如果对于某个速率rn······然后Tn→θ。

这直接从Slutsky定理出发，因为

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image034.gif)

如1/rn→0和rn（Tn−θ）→L Y。

这里是CLT和连续映射定理的一个简单结果假设X1，X2，。。。是

i.i.d平均μ和有限方差σ2的随机变量然后CLT说

.

然后，连续映射定理给出

.

# 3                三角法

Delta方法是关于分布收敛性的另一个一般性声明，与CLT结合使用时有着有趣的应用。

定理23.1（Delta方法）。如果√n（Tn-θ）L 2），则→n（0，τ

√-g（θ））→Ln（0，τ2（g0（θ））2）N（g（Tn）

G0（Th）存在且非零。

Delta方法非正式地指出，如果Tn具有极限正态分布，则g（Tn）也具有极限正态分布，并给出了g（Tn）渐近方差的显式公式这是令人惊讶的，因为g可以是线性的，也可以是非线性的一般来说，正态随机变量的非线性函数不具有正态分布。但是Delta方法是有效的，因为在假设下

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image035.gif)

当n（Tn＝Th）＝L n（0，Ta1 2）时，Tn＝pπ，使得Tn至少在大的N附近接近于Th，在一个邻域中，非线性函数G可以用线性函数逼近，这意味着G的行为类似于一个线性函数。事实上，delta方法是近似的结果：

g（Tn）－g（θ）≈g0（θ）（Tn－θ）。

下面是Delta方法的一个应用。

例23.2。假设0≤p≤1是一个固定参数，假设我们要估计p2。假设我们有两种估算p2的选择：

*1.*    我们可以通过X/n来估计p2，其中X是n个二项试验的成功次数，成功概率为p2。

*2.*    我们可以通过（Y/n）2估计p2，其中Y是n个二项试验中成功的次数，成功的概率为p。

以上哪一个是p2更好的估计量？为什么Delta方法为这个问题提供了一个简单的答案。注意，到CLT，我们已经

还有那个

Delta方法现在可用于将上述限制语句转换为（Y/n）2的精度语句，如下所示：

.

因此，我们推断（X/n）比（Y/n）2是p2的更好的估计量

p2（1-p2）<4p（1-p）p2

相当于p>1/3。因此，当p>1/3时，X/n比（Y/n）2是p2的更好的估计量，当p<1/3时，（Y/n）2是更好的估计量。

# 4                Delta方法在方差稳定变换中的应用

## 4.1              激励方差稳定转换

Delta方法可以应用于方差稳定变换例如，考虑这样一个例子，我们观察到数据X1、X2、…、Xn是具有Ber（p）分布的i.i.d。然后CLT说

（三十五）

不方便的是p也出现在方差项中。这在寻找p的置信区间时是一个麻烦。解决这个问题的一个方法是通过Slutsky定理来观察，

.

这是上节课做的。虽然这个方法是可以的，但是人们可能仍然怀疑是否有可能获得一个具有

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image037.gif)

当方差c2不依赖于p时，这样的函数f称为方差稳定变换。

对于另一个例子，考虑我们观察到的数据X1，…，Xn是i.i.d，具有Poi（λ）分布的情况然后CLT说

（三十六）

在寻找λ的置信区间时，在上述方差项中出现λ这一事实令人烦恼。正如上节课所做的，我们可以通过观察（通过Slutsky定理）来解决这个问题

.

虽然这个方法是可以的，但是人们可能仍然怀疑是否有可能获得一个具有

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image039.gif)

其中方差c2不依赖于λ。如果真的能找到这样一个f，则称之为方差稳定变换。

## 4.2              方差稳定变换的构造

更一般而言，考虑到结果：

√−θ）→Ln（0，τ2（θ））（37）N（Tn

当方差τ2（θ）依赖于θ时，是否有可能找到一个变换f

√–-f（θ））→L N（0，c2）（38）N（f（Tn）

其中方差c2不依赖于θ然后我们可以说函数f是一个方差稳定变换。

这可以通过Delta方法来实现。实际上，Delta方法表明

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image040.gif)

√-f（θ））→Ln（0，（f0（θ））2τ2（θ））N（f（Tn）

所以，为了保证（38），我们只需要选择f

（39个）

也就是说（不定积分）。

## 4.3              回到伯努利的例子

这里我们有X1，…，Xn，它是i.i.d，具有Ber（p）分布，因此通过CLT

√∏p）→L N（0，p（1−p））。n（Xn

因此（37）适用于Tn=X′n，θ=p和τ2（θ）=θ（1−θ）因此公式（38）表明我们选择f作为

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image042.gif)

√

也就是说f（θ）=2carcin（θ）。然后Delta方法保证

.

这意味着

作为n→∞

以便

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image044.gif)

√

约为100（1α）%C. I为p。上述区间的下端可以为负值（注：

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image045.gif)

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image046.gif)

√p’）取0和π/2之间的值，但arcin（pX∏n）–zα/2/（2√n）可以为负），而arcin（Xn

p总是正的。所以如果结果是负的，我们可以用0来代替下端点使用符号x+=max（x，0），我们看到

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image048.gif)

√

是一个近似100（1α）%C.I。对于P，得到P的置信区间，我们可以简单地对上述区间的两个端点进行平方。这让我们可以推断

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image050.gif)

是一个近似100（1α）%C。

## 4.4              回到泊松例子

现在让我们回到泊松分布，这里有X1，…，Xn是i.i.d Poi（λ），CLT给出

（36）。因此，Tn=X′n，θ=λ，τ2（θ）=θ方程（39）表明我们选择f作为

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image052.gif)

√

其中表示f（θ）=2cθ。然后Delta方法保证

（四十）

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image053.gif)

因此，应用于X′n的平方根变换确保（pX′n的）结果方差不依赖于λ（在极限意义上）。

事实（40）将导致近似的置信区间。确实，（40）立即意味着

作为n→∞

以便

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image055.gif)

√

是一个近似的100（1α）%C。注意，当λ始终为正时，上述区间的下端点可以为负。所以如果结果是负的，我们可以用0来代替下端点。

再次使用符号x+：=max（x，0），我们看到

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image057.gif)

√

是一个近似的100（1α）%C。为了得到λ的置信区间，我们可以简单地平方上述区间的两个端点。这让我们可以推断

（41个）

是一个近似的100（1α）%C。

这个区间可以与上节课用Slutsky定理得到的区间相比较。间隔时间是

（42）

间隔（41）和（42）看起来可能不同，但实际上它们对于大n来说彼此非常接近。要看到这一点，请注意这两个间隔的上界之间的差异最多为zα/22/（4n），当n较大时，这一差异非常小（下界也是如此）。

## 4.5              卡方检验

现在让我们看看另一个例子，其中的方差稳定转换是对数函数。

假设X1，X2，。。。i.i.d使得Xi/σ2具有一个自由度的卡方分布换句话说，

.

因为E（X1）=σ2和var（X1）=2σ4，CLT说

（43个）

我们现在能不能找到一个函数f，使得f（X′n）具有独立于σ2的极限方差因为（43）的形式是（37），其中√Tn=X∏n，θ=σ2，τ2（θ）=2θ2，所以我们可以使用（39）来表示取f so

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image058.gif)

即f0（θ）=c/τ（θ）=c/（2θ）这给了

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image060.gif)

让我们得出结论

.

平方根和对数是常见的转换，当存在变化的方差时应用于数据（参见，例如。

## 4.6              几何示例

假设X1，X2，。。。i.i.d是否具有参数p的几何分布。回想一下，如果X取1,2，…，X具有Geo（p）分布，。。。有可能

P{X=k}=（1-P）k-1p对于k=1,2，。。。。

获得第一个硬币头部所需的独立投掷次数（硬币头部概率为p）具有Geo（p）分布。

我离开是为了验证X∼Geo（p）

还有。

因此，CLT指出，对于i.i.d观测值X1，X2，。。。在地理分布上，我们有

.

什么是X′n的方差稳定变换，即f（X′n）具有恒定渐近方差的变换f是什么要回答这个问题，请注意，上面显示的方程与（37）的形式相同，Tn=X′n，θ=1/p，τ2（θ）=（1−p）/p2。然后我们用θas来写τ（θ）（注意p=1/θ）

.

因此，方差稳定变换由

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image062.gif)

√                                          √

前f（θ）=2clog（θ+θ-1）是这里的方差稳定变换

.

# 5                当g0（θ）=0时的Delta方法

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image063.gif)

假设√n（Tn−θ）→ln（0，τ2）我们现在对g（Tn）的渐近分布感兴趣Delta方法指出当g0（θ）6=0时

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image064.gif)

√-g（θ））→Ln（0，τ2（g0（θ））2）。（44）N（g（Tn）

这实质上是泰勒近似的结果：G（Tn）G（Th）G0（Th）（Tn×Th）。如果g0（θ）=0会发生什么在这种情况下，如果将右手侧解释为常数0，即当g0（θ）=0时，则语句（44）仍然正确，如下所示：

打√P

n（g（Tn）–g（θ））→0。

然而，这仅说明g（Tn）－g（θ）与n－1/2相比具有较小的阶数，但当按正确的阶数缩放时，并不能精确地说明确切的阶数和极限分布。为了解决这些问题，我们需要考虑g（Tn）在θ附近的泰勒展开式中的高阶项。在续集中，假设g0（θ）=0，g0（θ）6=0。

在这种情况下，我们做两项泰勒近似：

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image066.gif)

因此，我们

.

现在根据连续映射定理：

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image068.gif)

因此我们有

（四十五）

因此，当g0（θ）=0和g0（θ）6=0时，右标度因子为n，极限分布为的标度倍数（注意，极限不是正态分布）。

例25.1假设X1，X2，…，Xn是i.i.d Ber（p）随机变量假设我们要估计p（1-p）自然估计器是X′n（1−X′n）。这个估计器的极限行为是什么？

这可以用Delta方法通过取g（θ）=θ（1-θ）来回答。首先请注意，按照通常的CLT，

.

对于g（θ）=θ（1-θ），注意g0（θ）=1-2θ

所以当p 6=1/2时g0（p）6=0。因此，当p 6=1/2时，Delta方法给出

.

但是当p=1/2时，我们必须使用（45）而不是（44），这就给出了（注意g00（p）=-2）

.

# 6                条件作用

我们的下一个主题是条件作用这是统计类的一个非常重要的主题。

## 6.1              基础

首先让我们看看条件概率的定义。给定P（A）>0的两个事件A和B，我们将B的条件概率定义为

（46）

参见Jim Pitman 2016年201A笔记第10课第1.1节，以获得条件概率定义的一些直观理由。

使用这个条件概率的定义，我们可以看到

P（B）=P（B∩A）+P（B∩Ac）=P（B | A）P（A）+P（B | Ac）P（Ac）

注意这里A和Ac是不相交的事件，其联合是结果的整个空间。一般来说，如果A1，A2是不相交的事件，其联合是Ω，我们有

P（B）=XP（B | Ai）P（Ai）。（47）

i≥1

这被称为全概率定律。

现在让我们来看看贝叶斯规则

.

## 6.2              离散随机变量的条件分布、全概率律和贝叶斯规则

考虑两个随机变量X和Θ。假设两者都是离散随机变量然后，可以通过定义条件概率来定义给定的X的条件分布Θ=θ：

（48个）

假设P{Θ=θ}>0如果P{X=X |=θ}=0，我们就不会试图定义P{X=X |=θ}。

当x在随机变量x所取的所有值上变化时，概率（51）确定给定的x的条件分布Θ=θ。注意，条件概率P{X=X |=θ}总是在0和1之间，我们得到Px P{X=X |=θ}=1。

例26.1假设X和Y是分别具有Poi（λ1）和Poi（λ2）分布的独立随机变量对于n≥0，给定X+Y=n时X的条件分布是什么？

我们需要计算P{X=i | X+Y=n}

对于i的各种值，显然只有当i是介于0和n之间的整数时，上述概率才是非零的。因此，我们假设i是介于0和n之间的整数。根据定义

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image070.gif)

当X和Y分别独立分布为Poi（λ1）和Poi（λ2）时，可以直接计算上述分子。对于分母，我们使用X+Y是Poi（λ1+λ2）的事实（这个事实的证明留作练习）因此我们有

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image072.gif)

这意味着给定X+Y=n的X的条件分布是参数n和p=λ1/（λ1+λ2）的二项分布。

现在让我们来看看离散随机变量X和Θ的全概率定律和贝叶斯规则由于（47），我们有

P{X=X}=XP{X=X |=θ}P{X=θ}（49）

θ

其中，求和是随机变量Θ所取θ的所有值这个公式允许人们使用P{X=X |=θ}和P{X=θ}的知识来计算P{X=X}我们将（52）称为离散随机变量的总概率定律。

贝耶斯法则是

（五十）

Bayes规则允许计算给定X的条件概率，使用给定X的条件概率和边缘概率的知识。我们将（53）称为离散随机变量的Bayes规则。

例26.2假设N是具有Poi（λ）分布的随机变量同样假设，在N=N的条件下，随机变量X具有Bin（N，p）分布。此设置称为二项式的泊松化找到X的边际分布，也就是给定X=i的N的条件分布是什么？

为了找到X的边际分布，我们需要找到每一个i≥0的整数的P{X=i}为此，我们使用总概率定律，它指出

∞

P{X=i}=XP{X=i | N=N}P{N=N}。

n=0

因为X | N=N是Bin（N，p），只有当0≤i≤N时，概率p{X=i | N=N}才是非零的，因此只有当N≥i时，上述和中的项才是非零的，我们得到

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image074.gif)

这意味着X具有Poi（λp）分布。

为了找到给定X=i的条件分布，我们需要使用Bayes规则，它声明

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image076.gif)

当n≥i时，这仅为非零（否则P{X=i | n=n}将为零）。当n≥i时，我们有

n≥i时。

这意味着，在X=i的条件下，随机变量N以i+Poi（λ（1-p））的形式分布。

在这个例子中，X和N-X的联合分布是什么要计算这个，请注意

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image078.gif)

注意，这将分解成只涉及i的项和只涉及j的项。因此，这意味着X和N-X是独立的同样从上面的表达式，很容易推断X的边缘分布是Poi（λp）（我们已经通过总概率定律导出了该分布），N-X是Poi（λ（1-p））。

这个例子的背景是当一个人掷硬币的概率为p时，硬币的头部独立于Poi（λ）的次数然后，N表示投掷的总数，X表示头部的数量，N-X表示尾部的数量。因此，我们证明了X和N-X是独立的，并且分别根据Poi（λp）和Poi（λ（1-p））分布。这里X和N-X的独立性特别有趣当一枚硬币被投掷固定的n次时，正面和反面的数量显然是不独立的（因为它们必须和n相加）。但当抛掷次数本身是随机的且具有泊松分布时，则头部和尾部的数目成为独立的随机变量。

# 7                离散随机变量的条件

在上一课中，我们研究了离散随机变量的条件作用给定两个离散随机变量X和Θ，然后可以通过定义条件概率来定义X的条件分布给定Θ=θ：

（51个）

假设P{Θ=θ}>0如果P{X=X |=θ}=0，我们就不会试图定义P{X=X |=θ}。

当x在随机变量x所取的所有值上变化时，概率（51）确定给定的x的条件分布Θ=θ。注意，条件概率P{X=X |=θ}总是在0和1之间，我们得到Px P{X=X |=θ}=1。

我们还研究了全概率定律和贝叶斯规则总概率定律是

P{X=X}=XP{X=X |=θ}P{X=θ}（52）

θ

其中，求和是随机变量Θ所取θ的所有值这个公式允许人们使用P{X=X |=θ}和P{X=θ}的知识来计算P{X=X}贝耶斯法则是

（五十三）

Bayes规则允许计算给定X的条件概率，使用给定X的条件概率和边缘概率的知识。

本课程的目标是将以上所有内容扩展到X和Θ是连续随机变量的情况。

# 8                连续随机变量的条件密度

现在考虑两个连续的随机变量X和Θ有一个联合密度fX，Θ（X，θ）回想一下，对于所有x，θ和RR f（x，θ）dxdθ=1，fX，Θ（x，θ）≥0。还记得X和Θ的边缘密度是由

和

我们现在定义X的条件密度，对于θ的固定值，给定Θ=θ。为了在x点定义条件密度，我们需要考虑

P{x≤x≤x+δ|=θ}（54）

对于较小的δ>0。因为P{Θ=θ}=0（注意，Θ是一个连续的随机变量），我们不能用定义P（B | a）：=P（B∩a）/P（a）来定义这个条件概率但是，直觉上，条件作用

Θ=θ应等于对小的进行调节所以我们可以写

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image080.gif)

小的。对于上面右边的概率，我们可以使用P（B | A）：=P（B∩A）/P（A）来获得

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image082.gif)

我们就这样得到了

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image084.gif)

对于小δ这表明了

（55个）

对于给定的X的条件密度Θ=θ。只要fΘ（θ）>0，这个定义就有意义如果fΘ（θ）=0，我们不试图定义fX|920;=θ。

例28.1假设X和Θ分别是具有Gamma（α，λ）和Gamma（β，λ）分布的独立随机变量那么给定X+Θ=1，X的条件密度是多少。

定义（55）给出

.

通过计算转换随机变量密度的雅可比公式，可以检验

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image086.gif)

对于0<x<1我们以前也看到X+Θ分布为Γ（α+β，λ）因此

.

因此

对于0<x<1。

因此，这意味着

X |（X+Θ=1）∼β（α，β）。

例28.2假设X和Y是独立的Unif（0,1）随机变量。什么是fU | V=V，其中U=min（X，Y）和V=max（X，Y）和0<V<1？

首先要注意

.

当0<u<v<1时，我们知道

fU，V（u，V）=fX，Y（u，V）+fX，Y（V，u）=2。

同时V=最大值（X，Y）∼β（2，1），以便

fV（v）=2vI{0<v<1}。

因此我们有

对于0<u<v。

换句话说，U | V=V在区间（0，V）上均匀分布。

# 9                条件密度与接头密度成正比

条件密度

（56个）

具有以下重要特性。作为x的函数（并保持θ不变），fX |Θ=θ（x）是有效密度，即。，

Z∞

fX |Θ=θ（x）≥0，fX |Θ=θ（x）dx=1。

−∞

上面的积分等于1，因为

.

因为f x |Θ=θ（x）作为x的函数积分到一，并且因为定义（56）中的分母fΘ（θ）不依赖于x，所以通常写入

fX |Θ=θ（x）ΘfX，Θ（x，θ）。（57）

这里的符号表示“与”成比例，上面的说法是，fX |Θ=θ（x），作为x的函数，与fX，Θ（x，θ）成比例。比例常数必须是fΘ（θ），因为这等于f x的积分值，当x的范围超过（－～～，∞）时，x（x，θ）的积分值。

比例陈述（57）通常使得涉及条件密度的计算简单得多。为了说明这一点，让我们分别回顾示例（28.1）和（28.2）中的计算。

例29.1（例28.1回顾）。假设X和Θ分别是具有Gamma（α，λ）和Gamma（β，λ）分布的独立随机变量那么X的条件密度是多少

X+Θ=1？在（57）之前，

fX | X+Θ=1（X）∮fX，X+Θ（X，1）

=fX，Θ（x，1-x）

=fX（x）fΘ（1-x）

{e∏λx xα-1I{x>0}e∏λ（1-x）（1-x）β-1I{1-x>0}xα-1（1-x）β-1I{0<x<1}

这立即意味着X | X+Θ=1具有参数α和β的β分布。

例29.2（例28.2回顾）。假设X和Y是独立的Unif（0,1）随机变量。什么是fU | V=V，其中U=min（X，Y）和V=max（X，Y）和0<V<1？

写

fU | V=V（u）| fU，V（u，V）

=2fU（u）fV（v）I{u<v}

{fU（u）I{u<v}

=I{0<u<1}I{u<v}=I{0<u<min（v，1）}

因此，对于v<1，给定v=v的条件密度是[0，v]上的均匀密度。对于v>1，给定v=1时U的条件密度不定义为v>1时v的密度等于0。

# 10          条件密度与独立性

*X*    且Θ是独立的，当且仅当fX |Θ=θ=fX对于θ的每个值后一种说法正好等于f x，Θ（x，θ）=fX（x）fΘ（θ）。通过交换X和Θ的角色，也可以得出X和Θ是独立的，当且仅当fΘ| X=X=fΘ对于每个X。

当且仅当给定的X的条件密度对fΘ（θ）>0的所有θ值相同时，也不难看出X和Θ是独立的。

示例30.1（回到Gamma示例）我们之前已经看到，当X∼γ（α，λ）和

*Y*    ∼伽马（β，λ），然后

X |（X+Θ=1）∼β（α，β）。

这也可以直接看到（通过观察X/（X+920）分布为β（α，β），X/（X+920）独立于X+920），如下所示：

.

注意，在上面的最后一步中，我们去掉了X+Θ=1上的条件作用，因为X/（X+Θ）与X+Θ无关。