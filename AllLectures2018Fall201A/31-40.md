# 1                连续随机变量的全概率律和贝叶斯规则

现在让我们回到条件密度的性质。

\1.    根据fX |Θ=θ（x）的定义，它直接如下

f x，Θ（x，θ）=fX |Θ=θ（x）fΘ（θ）。

这就告诉我们如何利用已知的边值和条件密度来计算X和Θ的联合密度。

\2.    连续随机变量的总概率定律：回顾（52）中离散随机变量的总概率定律。连续随机变量的类似语句是

Z f x（x）=fX |Θ=θ（x）fΘ（θ）dθ。

这直接来自fX |Θ=θ（x）的定义这个公式允许我们利用给定的X的条件密度和X的边际密度的知识来推导X的边际密度。

\3.    连续随机变量的Bayes规则：回顾（53）中离散随机变量的Bayes规则连续随机变量的类似语句是

.

这允许使用给定X的条件密度和边缘密度的知识来推导给定X的条件密度。

# 2                连续随机变量的全概率律和贝叶斯规则

假设X和Θ是具有联合密度fX的连续随机变量。我们已经看到了

.

注意上面右边的分母不包含x，所以我们可以写

fX |Θ=θ（x）ΘfX，Θ（x，θ）。

注意我们也有

fΘ| X=X（θ）ΘfX，Θ（X，θ）。

以下是条件密度定义的直接结果。

一我们有f x，Θ（x，θ）=fX |Θ=θ（x）fΘ（θ）。2。总概率定律指出

Z f x（x）=fX |Θ=θ（x）fΘ（θ）dθ

三。Bayes规则指出

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.gif)

这里有两个例子来说明这些。

例32.1。假设Θ∼N（μ，τ2）和X |Θ=θ∼N（θ，σ2）那么X的边际密度和X=X的条件密度是多少？

为了得到X的边缘密度，我们用LTP表示

Z f x（x）=fX |Θ=θ（x）fΘ（θ）dθ

现在

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.gif)

指数中的项可以简化为

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image006.gif)

在这里，我跳过了几个步骤来获得最后的等式（完成正方形并简化得到的表达式）。

因此

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image008.gif)

因此，

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.gif)

它给予

X∼N（0，τ2+σ2）。

为了得到fΘ| X=X（θ），我们使用Bayes规则：

!

也就是说

.

对于均值m和方差v2的正态密度，方差1/v2的逆称为精度。因此，上述计算表明，给定X的条件分布的精度分别等于X的分布和X的分布的精度之和。

在统计术语中，通常称为：

*1.*    作为未知参数θ的先验分布的边缘分布。

*2.*    X |Θ=θ的条件分布，作为以真参数值为条件的数据分布。

*3.*    在给定数据的情况下，X=X的条件分布作为X的后验分布。

在这个特定的例子中，后验分布的平均值是先验平均值以及权重与精度成正比的数据的加权线性组合而且，后验精度等于先验精度和数据精度之和，这在非正式意义上特别意味着后验精度比先验精度更高。

例32.2。设Θ∼γ（α，λ）和X |Θ=θ∼Exp（θ）。X的边际密度是多少，给定X=X的条件密度是多少？

对于X的边缘，使用LTP：

.

这称为Lomax分布，形状参数α>0，比例/速率参数λ>0（请参见。

对于给定X=X的条件分布，通过比例论证

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image012.gif)

也就是说

Θ| X=X∼伽马（α+1，λ+X）。

# 3                一般随机变量的LTP和Bayes规则

LTP描述了如何根据给定的X的条件分布和条件分布来计算X的分布。Bayes规则描述了如何在已知X的条件分布和X的条件分布的基础上，计算给定X=X的条件分布。到目前为止，我们研究了当X和Θ都是离散的或它们都是连续的时的LTP和Bayes规则。现在我们还要考虑一个是离散的，另一个是连续的情况。

## 3.1              X和Θ都是离散的

在这种情况下，我们已经看到LTP是

P{X=X}=XP{X=X |=θ}P{X=θ}

θ

贝耶斯法则是

.

## 3.2              X和Θ都是连续的

这里是LTP

贝耶斯法则是

.

## 3.3              X是离散的，而Θ是连续的

LTP为Z

P{X=X}=P{X=X |=θ}fΘ（θ）dθ

贝耶斯法则是

.

## 3.4              X是连续的，而Θ是离散的

LTP是

fX（x）=XfX |Θ=θ（x）P{Θ=θ}

θ

贝耶斯法则是

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image014.gif)

当X给定的条件分布Θ=θ以及Θ的边际分布易于确定（或作为模型规范的一部分给出）并且目标是确定X的边际分布以及X给定的条件分布Θ=X时，这些公式是有用的。

我们现在来看看当X和Θ中的一个是离散的，而另一个是连续的时，LTP和Bayes规则的两个应用。

例33.1。假设Θ是（0,1）上的均匀分布，且设X|=θ具有参数n和θ的二项式分布（即，在Θ=θ的条件下，随机变量X以成功概率θ的硬币n次独立投掷中的成功次数分布）那么X的边际分布和给定X=X的条件分布是什么？

注意，在这种情况下，X是离散的（取0,1，…，n中的值），而Θ是连续的（取区间（0,1）中的值）为了计算X的边际分布，我们使用适当的LTP来写（对于X=0,1，…，n）

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image016.gif)

这意味着X在有限集{0,1，…，n}上均匀分布。

现在让我们计算给定X=X的后验分布。使用Bayes规则，我们得到

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image018.gif)

对于0<θ<1。从这里开始，紧接着

Θ| X=X∼β（X+1，n−X+1）。

β（α，β）分布的平均值为α/（α+β）。因此，给定X=X的条件分布的平均值（也称为后验平均值）等于

.

因为前面的平均值等于1/2，我们可以写

,

因此，后验平均值介于前验平均值和x/n之间。当n变大时，后验平均值接近x/n。

例33.2（统计分类）在统计分类问题中，随机变量Θ是离散的，X通常是连续的。最简单的情况是当Θ是二进制时让我们这么说吧

P{Θ=1}=P和P{Θ=0}=1−P。

还假设X给定的条件密度为f0，X给定的条件密度为f1，即。，

X | 920;=0∼f0和X | 920;=1∼f1。

使用LTP，我们看到X的边缘密度等于

fX=（1−p）f0+pf1。

换言之，FX是F0和F1的混合，其混合权重等于β的边际概率。

根据Bayes规则，给定X=X的条件分布由

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image020.gif)

和

.

这些也被称为给定X=X的后验概率。

# 4                条件节理密度

给定连续随机变量X1，…，Xm，Y1，…，Yk，条件联合密度Y1，…，Yk

X1=X1，X2=X2，…，Xm=Xm定义为

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image022.gif)

假设x1，…，xm是这样的：fX1，…，xm（x1，…，xm）>0。

下面是条件连接密度的一些简单但重要的性质。

\1.    对于每个x1，…，xm，y1，…，yk，我们都有

fX1，…，Xm，Y1，…，Yk（x1，…，Xm，Y1，…，Yk）=fY1，…，Yk | x1=x1，…，Xm=Xm（Y1，…，Yk）fX1，…，Xm（x1，…，Xm）。

\2.    每组随机变量Y1，…，Yn的联合密度满足以下条件：fY1，…，Yn（Y1，…，Yn）=fYn | Y1=Y1，…，Yn−1=Yn−1（Yn）fYn−1 | Y1=Y1，…，Yn−1=Yn−2（Yn−1）…fY2 | Y1=Y1（y2）fY1（Y1）。

\3.    这是先前事实的概括。条件关节密度

fY1，…，Yn | X1=X1，…，Xm=Xm（y1，…，Yn）

Y1，…，Yn给定X1=X1，…，Xm=Xm等于乘积

n个

是的

仅供参考：Y1=Y1，…，Yi-1=Yi-1，X1=X1，…，Xm=Xm（Yi）i=1

\4.    这可以看作是总条件概率定律：对于随机变量Y1，…，Yk，X1，…，Xm和Θ，我们有

Z轴

fY1，…，Yk | X1=X1，…，Xm=Xm（y1，…，Yk）=fY1，…，Yk，Θ| X1=X1，…，Xm=Xm（y1，…，Yk，θ）fΘX1=X1，…，Xm=Xm（θ）dθ。

我们将在下一节课上讨论上述事实的一些应用。

# 5                条件节理密度

给定连续随机变量X1，…，Xm，Y1，…，Yk，条件联合密度Y1，…，Yk给定X1=X1，X2=X2，…，Xm=Xm定义为

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image024.gif)

假设x1，…，xm是这样的：fX1，…，xm（x1，…，xm）>0。

例35.1假设U1，…，Un是在（0,1）上具有均匀密度的独立观测值。当U（n）=U时，U（1），…，U（n-1）的条件连接密度是多少根据定义，

.

根据我们之前计算出的订单统计量的联合分布，首先，只有当0<u1<····················

.

对于上面的分母，我们使用了U（n）∼Beta（n，1）这个事实我们已经证明了

对于0<u1<····<un-1<u<1。

注意，上面的右手边是从区间（0，u）上的均匀分布得出的（n-1）i.i.d观测值的阶统计量的联合密度。因此，我们证明了，在U（n）=U的条件下，U（1），…，U（n-1）的联合密度与（0，U）上均匀分布的（n-1）i.i.d观测的阶统计量的联合密度相同。

下面是条件连接密度的一些简单但重要的性质。

\1.    对于每个x1，…，xm，y1，…，yk，我们都有

fX1，…，Xm，Y1，…，Yk（x1，…，Xm，Y1，…，Yk）=fY1，…，Yk | x1=x1，…，Xm=Xm（Y1，…，Yk）fX1，…，Xm（x1，…，Xm）。

\2.    每组随机变量Y1，…，Yn的联合密度满足以下条件：fY1，…，Yn（Y1，…，Yn）=fYn | Y1=Y1，…，Yn−1=Yn−1（Yn）fYn−1 | Y1=Y1，…，Yn−1=Yn−2（Yn−1）…fY2 | Y1=Y1（y2）fY1（Y1）。

\3.    这是先前事实的概括。条件关节密度

fY1，…，Yn | X1=X1，…，Xm=Xm（y1，…，Yn）

Y1，…，Yn给定X1=X1，…，Xm=Xm等于乘积

n个

是的

仅供参考：Y1=Y1，…，Yi-1=Yi-1，X1=X1，…，Xm=Xm（Yi）i=1

\4.    这可以看作是总条件概率定律：对于随机变量Y1，…，Yk，X1，…，Xm和Θ，我们有

Z轴

fY1，…，Yk | X1=X1，…，Xm=Xm（y1，…，Yk）=fY1，…，Yk，Θ| X1=X1，…，Xm=Xm（y1，…，Yk，θ）fΘX1=X1，…，Xm=Xm（θ）dθ。

以下是上述事实的一些应用。

例35.2（自回归过程的联合密度）假设X1，Z2，…，Zn是独立随机变量，Z2，…，Zn分布为N（0，σ2）。定义新的随机变量X2，…，Xn via

Xi=φXi-1+Zi，对于i=2，…，n

其中φ是一个实数进程X1，…，Xn被称为顺序为1的自回归进程给定X1=X1，X2，…，Xn的条件连接密度是多少？X1，…，Xn的连接密度是多少让我们首先计算条件连接密度X2，…，Xn，给定X1=X1。为此，写下

n个

Fx2，…，Xn x1＝x1（x2，…，xn）＝yfxi x1= x1，…，Xi＝1＝Xi＝1（Xi）（58）

i=2

现在对于每个i=2，…，n，请注意

Xi x1= x1，…，Xi＝1＝Xi＝1＝（DωXi＝1＋Zi）x1＝x1，…，Xi＝1＝Xi＝1。

丁

=（πXi＝1＋Zi）x1= x1，…，Xi＝1＝Xi＝1

第二天

=ω- 1＋Zi~（n）（πXi，1，Sig.）。

我们可以去除X1= x1，…，Xi＝Xi＝1以上的条件，因为X1，…，Xi＝1只依赖于X1，Z2，…，Zi，1，因此独立于ZI。

从上面的断言链，我们推断

对于i=2，…，n。

结合（58），我们得到

.

要获得X1，…，Xn，write的连接密度

.

在统计设置中，利用该联合密度通过最大似然估计来估计参数ω和2。然而，对于该模型，使用条件密度X2，…，Xn（给定X1=X1）而不是全关节密度X1，…，Xn更容易。

## 5.1              正态先验正态数据模型的应用

现在让我们来看看条件密度公式在正态先验正态数据模型中的应用这里我们首先得到一个具有N（μ，τ2）分布的随机变量Θ。我们也有随机变量X1，…，Xn+1这样

X1，…，Xn+1 |Θ=θ∼i.i.d N（θ，σ2）。

换句话说，以Θ=θ为条件，随机变量X1，…，Xn+1为i.i.d N（θ，σ2）。

让我们首先找到给定X1=X1，…，Xn=Xn的条件分布答案是

（59个）

式中，xn：=（x1+···+xn）/n。让我们看看下面为什么是这样首先请注意，我们在上一节课中已经解决了n=1的这个问题，我们证明了：

.

一般n≥1的结果（59）实际上可以从n=1的上述结果中导出。有两种方式可以看到这一点。

方法一：对n≥1采用数学归纳法。我们已经知道（59）对于n=1是正确的。假设n是真的，我们将试图证明n+1是真的。关键是要注意

（Θ| X1=X1，…，Xn+1=Xn+1）=dΘ| Y=Xn+1（60）

哪里

Y |Θ=∼θ∼N（θ，σ2）和Θ∼∼X1=X1，…，Xn=Xn。

也就是说，（60）表示在观测（n+1）观测值X1=X1，…，Xn+1=Xn+1后的后验与在先前的观测值X1=X1，…，Xn=Xn下观测一个观测值Y=Xn+1后的后验相同。

要正式了解为什么（60）是真的，请注意

fΘX1=X1，…，Xn=Xn，Xn+1=Xn+1（θ）≤fXn+1 | X1=θ，X1=X1，…，Xn=Xn（Xn+1）fΘX1=X1，…，Xn=Xn（θ）

=f xn+1 | X1=X1，…，xn=xn（θ）。

第一等式是条件密度性质的结果。上面的第二个等式是Xn+1独立于X1，…，Xn条件为Θ的结果。

语句（60）允许我们使用n=1的结果和（59）适用于n的归纳假设

和

并且x=xn+1，我们推断出Θ| X1=X1，…，xn+1=xn+1是具有平均值的正态分布

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image026.gif)

和变化

.

这证明了n+1的（59）（59）的证明是通过归纳法完成的。

方法二证明（59）的第二种方法通过书写更直接地进行：

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image028.gif)

这与我们之前对n=1的计算类似。唯一的区别是x现在被∏x n代替，而σ2被σ2/n代替。因此，应用于x→x∏n和σ2→σ2/n的n=1结果应产生（59）这证明了（59）。

现在让我们计算Xn+1的条件密度，给定X1=X1，…，Xn=Xn为此，我们可以用总条件概率定律来写

Z轴

f Xn+1 | X1=X1，…，Xn=Xn（x）=fXn+1 | X1=θ，X1=X1，…，Xn=Xn（x）fΘX1=X1，…，Xn=Xn（θ）dθ

Z轴

=f Xn+1 |Θ=θ（x）fΘX1=X1，…，Xn=Xn（θ）dθ

这又类似于n=1问题中X的边缘密度的计算（其中答案是X∼n（μ，τ2+σ2））。唯一的区别是先前的N（μ，τ2）现在被（59）给出的后验密度所代替因此，我们得到

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image030.gif)

# 6                条件期望

给定两个随机变量X和Y，给定X=X的Y的条件期望（或条件平均值）表示为

E（Y | X=X）

定义为给定X=X时Y的条件分布的期望值。

我们可以写

Y是连续的

：如果Y是离散的

更普遍地说

R g（y）fY | X=X（y）dy:如果y是连续的

E Y X：如果Y是离散的

还有

Y是连续的

：如果Y是离散的

关于条件期望，最重要的事实是迭代期望定律（也称为总期望定律）。我们下次再看。

## 6.1              迭代/总期望定律

总期望定律指出

X是连续的

（）=P X E（Y | X=X）P{X=X}：如果X是离散的

基本上，总期望定律告诉我们如何利用给定X=X的条件期望的知识来计算E（Y）的期望。注意与总概率定律的相似性，总概率定律规定了如何利用给定Y的条件分布的知识来计算Y的边际分布

X=X。

总期望定律可以作为总概率定律的结果来证明。下面给出Y和X是连续的证明在其他情况下（当Y和X中的一个或两个是离散的）的证明是相似的，作为一个练习。

总期望定律的证明：假设Y和X都是连续的那么

Z E（Y）=yfY（Y）dy。根据全概率定律，我们有

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image032.gif)

证明了总期望定律。

有一种更简洁的形式来表述总期望定律，这使得我们有理由称之为迭代期望定律我们下次再看。注意E（Y | X=X）依赖于X。换句话说，E（Y | X=X）是X的函数。让我们用h（·）表示这个函数：

h（x）：=E（Y | x=x）。

如果我们现在把这个函数应用到随机变量X，我们得到一个新的随机变量h（X）。这个随机变量用简单的E（Y | X）表示，即。，

E（Y | X）：=h（X）。

注意，当X是离散的时，这个随机变量E（Y | X）的期望变成

.

当X是连续的，E（Y | X）的期望值是

Z Z轴

E（E（Y | X））=E（h（X））=h（X）fX（X）dx=E（Y | X=X）fX（X）dx。

观察到，这些期望中的右手边正是总期望定律中右手边的术语。因此，总期望定律可以重新表述为

E（Y）=E（E（Y | X））。

因为在右手边有两个期望，所以总期望定律也被称为迭代期望定律。

迭代期望定律有许多应用下面给出几个简单的例子，我们将探讨风险最小化的应用。

例36.1。考虑一根长的棍子。在一个随机点X处断开它，该点被均匀地选在杆的长度上。然后在任意点Y再次折断木棍，该点也均匀地选择在木棍长度上。最后一块的预期长度是多少？

根据问题的描述，

Y | X=X∼Unif（0，X）和X∼Unif（0，`）

我们需要计算E（Y）首先注意E（Y | X=X）=X/2，这意味着

E（Y | X）=X/2。因此根据迭代期望定律，

E（Y）=E（E（Y | X））=E（X/2）=`/4。

## 6.2              迭代/总期望定律

在上一节课中，我们研究了迭代期望定律（或总期望定律），它指出

E（Y）=E（E（Y | X））。

在上面的右侧，E（Y | X）是通过将函数h（X）：=E（Y | X=X）应用于随机变量X（即E（Y | X）=h（X））而获得的随机变量。

迭代期望定律有许多应用下面给出几个简单的例子，我们将探讨风险最小化的应用。

例36.2。假设X，Y，Z是i.i.d Unif（0,1）随机变量找到P{X≤Y Z}的值根据迭代期望定律，

P{X≤Y Z}=E（I{X≤Y Z}）=E[E（I{X≤Y Z}Y Z]=E（Y Z）=E（Y）E（Z）=1/4。

例36.3（i.i.d随机变量的随机数之和）假设X1，X2，。。。是i.i.d随机变量，E（Xi）=μ还假设N是一个离散随机变量，它接受{1,2，…，}中的值，并且它独立于X1，X2定义

S:=X1+X2+····+XN。

换句话说，S是随机变量Xi的随机数（N）的和迭代期望定律可用于计算S的期望值，如下所示：

E（S）=E（E（S | N））=E（Nμ）=（μ）（EN）=（EN）（EX1）。

这个事实实际上是一个称为Wald的身份的一般结果的特例。

## 6.3   总期望定律在统计风险最小化中的应用

迭代期望律在统计风险最小化问题中有着重要的应用。其中最简单的问题如下。

问题1：给定两个随机变量X和Y，X的函数g∗（X）是什么使

R（g）：=E（g（X）－Y）2

所有的功能由此产生的随机变量g∗（X）可以被称为Y的最佳预测因子，作为X的函数，用期望平方误差表示。

为了找到g*，我们使用迭代期望定律来编写

R（g）=E（g（X）－Y）2=EnEh（g（X）－Y）2 | Xio

使内部期望最小化的值g∗（x）：

EH（Y·G（x））2×x= Xi

简单地说就是g∗（x）=E（Y | x=x）。

这是因为当c在c∗=E（Z）时c在R上变化时，E（Z-c）2最小化。因此，我们证明了在所有函数g上最小化R（g）的函数g∗（X）是由

g∗（X）=E（Y | X。

因此，就期望平方误差而言，X的函数最接近Y，由条件平均E（Y | X）给出。

现在让我们考虑一个不同的风险最小化问题。

问题2：给定两个随机变量X和Y，X的函数g∗（X）是什么使

R（g）：=E | g（X）–Y|

所有的功能由此产生的随机变量g∗（X）可以被称为Y的最佳预测因子，作为X的函数，用期望绝对误差表示。

为了找到g*

R（g）=E | g（X）–Y |=E{E[| g（X）–Y | X]}

使内部期望最小化的值g∗（x）：

E[| Y−g（x）| x=x]

简单地由给定X=X的Y的任何条件中值给出。这是因为当c在Z的任何中值处变化超过R时，E | Z−c |最小化。为此，假设Z具有密度f并写入

Z轴

E | Z−c |=| Z−c | f（Z）dz

Z c Z∞

=（c-z）f（z）dz+（z-c）f（z）dz

-∞c

Z c Z c Z∞Z∞

=c f（z）dz-zf（z）dz+zf（z）dz-cf（z）dz。

－～－～∞控制中心

关于c的区别，我们得到

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image034.gif)

因此，当c是中值时，E | Z−c |的导数将等于零这表明，当c是Z的中值时，c 7→E | Z−c |最小化。

因此，我们证明了在所有函数g上最小化R（g）的函数g∗（x）是由给定x=x的任何条件平均值给出的。因此，给定x=x的条件平均值是就预期绝对误差而言最接近Y的x的函数。

问题3：假设Y是一个二进制随机变量，取值0和1，X是任意随机变量。X的函数g∗（X）是什么使

R（g）：=P{Y 6=g（X）}

所有的功能为了解决这个问题，再次使用迭代期望定律来编写

R（g）=P{Y 6=g（X）}=E（P{y6=g（X）| X}）。

在上面的内部期望中，我们可以把X当作一个常数，使得这个问题类似于二元随机变量Z在c∈R上最小化P{Z 6=c}，很容易看出P{z6=c}在c*处最小化，其中

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image036.gif)

如果P{Z=1}=P{Z=0}，我们可以将c*取为0或1从这里可以（通过迭代期望定律）推断出最小化P{Y 6=g（X）}的函数g∗（X）由

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image038.gif)

问题4：再次假设Y是二进制的，取值为0和1，X是任意随机变量X的函数g∗（X）是什么使

R（g）：=W0P{Y 6=g（X），Y=0}+W1P{y6=g（X），Y=1}。

使用类似于前面问题的参数，推断以下函数使R（g）最小化：

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image040.gif)

在上述四个问题中使用的论点（通过迭代期望定律）可以总结如下。使

R（g）：=EL（Y，g（X））

所有函数g由

g∗（x）=c∈R上E（L（Y，c）| x=x）的最小值。

# 7                条件方差

给定两个随机变量Y和X，Y给定X=X的条件方差被定义为Y给定X=X的条件分布的方差，

.

与条件期望一样，条件方差V ar（Y | X=X）也是X的函数，我们可以把这个函数应用到随机变量X上，得到一个新的随机变量，我们用V ar（Y | X）表示。注意

V ar（Y | X）=E（Y 2 | X）－（E（Y | X））2。（61）

与总期望定律类似，也有总方差定律这个公式说明

V ar（Y）=E（V ar（Y | X））+V ar（E（Y | X））。

为了证明这个公式，把右手边展开为

E（V ar（Y | X））+V ar（E（Y | X））=EnE（Y 2 | X）－（E（Y | X））2o+E（E（Y | X））2－（E（E（Y | X））2

=E（E（Y 2 | X））-E（E（Y | X））2+E（E（Y | X））2-（E（Y））2=E（Y 2）-（EY）2=V ar（Y）。

例37.1我们以前见过

X |Θ=θ∼N（θ，σ2）和ΘN（μ，τ2）=X∼N（μ，σ2+τ2）。

当然，这意味着

E（X）=μ，V ar（X）=σ2+τ2。

利用总期望和总方差定律，可以直接证明如下。

E（X）=E（E（X | 920;））=E（Θ）=μ

和

V ar（X）=E（V ar（X |Θ））+V ar（E（X |Θ））=E（σ2）+V ar（Θ）=σ2+τ2。

例37.2（i.i.d随机变量的随机数之和）。假设X1，X2，。。。是i.i.d随机变量，E（Xi）=μ，V ar（Xi）=σ2<∞还假设N是一个离散随机变量，它接受{1,2，…，}中的值，并且它独立于X1，X2定义

S:=X1+X2+····+XN。

我们以前见过

E（S）=E（E（S | N））=E（Nμ）=（μ）（EN）=（EN）（EX1）。

利用总方差定律，我们可以计算V-ar（X）如下。

V ar（S）=E（V ar（S | N））+V ar（E（S | N））=E（Nσ2）+V ar（Nμ）=σ2（EN）+μ2V ar（N）。

.

# 8                随机向量

在这一节中，我们将有限个随机变量看作一个随机向量，并讨论了随机向量均值和协方差的一些基本公式。

随机向量是一个向量，其条目是随机变量。设Y=（Y1，…，Yn）T为随机向量它的期望EY被定义为一个向量，其第i项是Yi的期望，即EY=（EY1，EY2，…，EYn）T。

Y的协方差矩阵，用Cov（Y）表示，是一个n×n矩阵，其（i，j）项是Yi和Yj之间的协方差。关于Cov（Y）的两个重要但简单的事实是：

\1.    Cov（Y）的对角线项是Y1，…，Yn的方差更具体地说，矩阵Cov（Y）的第（i，i）项等于var（Yi）。

\2.    Cov（Y）是一个对称矩阵，即Cov（Y）的第（i，j）项等于（j，i）项这是因为Cov（Yi，Yj）=Cov（Yj，Yi）。

以下公式很简单，但非常有用。他们的证据留作练习。

\1.    对于每个确定性矩阵A和每个确定性向量c，E（A Y+c）=AE（Y）+c。

\2.    Cov（A Y+c）=每个确定性矩阵A和每个确定性向量c的ACov（Y）。

这里有一个例子说明了这些公式的使用。

例38.1对于长度为n（或n×1阶）的向量a和n×1随机向量Y，我们有

V ar（aTY）=Cov（aTY）=aTCov（Y）a.（62）

这是写公式的简洁方法

.

注意（62）是公式Cov（a Y+c）=ACov（Y）AT的直接结果。

# 9                随机向量

在上一节课中，我们开始研究随机向量一个p×1随机向量Y由p个随机变量Y1，…，Yp即Y=（Y1，…，Yp）T组成。

平均向量Y由EY=（EY1，…，EY p）T给出，Y的协方差矩阵由p×p矩阵给出，p×p矩阵的（i，j）项等于Yi和Yj之间的协方差注意Cov（Y）的对角线项是Y1，…，Yp的方差还要注意Cov（Y）是一个对称矩阵。

上节课我们还学习了以下两个重要公式：

\1.    对于每个确定性矩阵A和每个确定性向量c，E（A Y+c）=AE（Y）+c。

\2.    Cov（A Y+c）=每个确定性矩阵A和每个确定性向量c的ACov（Y）。

例39.1（白噪声）如果随机变量Z1，…，Zp的均值为零，方差为1且不相关，则称其为白噪声。设Z为分量为Z1，…，Zp的随机向量当且仅当EZ=0且Cov（Z）=Ip（这里Ip是p×p恒等式矩阵）时，Z的分量是白噪声。

作为上面第二个公式的结果，我们看到

var（aTY）=aTCov（Y）a，对于每个p×1向量a。

因为方差总是非负的，这意味着Cov（Y）满足以下性质：

aTCov（Y）a=var（aTY）≥0，对于每个p×1向量a。（63）

现在回想一下线性代数的以下定义：

定义39.2。设∑表示p×p对称矩阵。

*1.*    当aT∑a≥0时∑为正半正定。

*2.*    如果在∑a>0时，对于a∈Rp，a为6=0，则∑为正定。

从这个定义和事实（63）可以看出，每个随机向量Y的协方差矩阵Cov（Y）是对称的和半正定的。

然而Cov（Y）不一定是正定的。要了解这一点，只需对随机变量Y1取p=2和Y=（Y1，-Y1）T当a=（1,1）时，很容易看出aTCov（Y）a=vr（aTY）=vr（Y1+Y2）=0。

但是如果Cov（y）不是正定的，则存在6＝0，使得V AR（ATY）＝ATCOV（y）a＝0。这必然意味着在（Y-μ）=0时，其中μ=E（Y）。换句话说，随机变量Y1，…，Yn必须满足一个线性方程。因此我们可以说：Cov（Y）是正定的，当且仅当随机变量Y1，…，Yn不满足线性方程。

# 10          对称矩阵的绕道谱定理

由于Cov（Y）是一个对称的半正定矩阵，因此在处理协方差矩阵时，关于这类矩阵的一些标准事实是有用的特别地，我们将利用对称矩阵的谱定理。在研究谱定理之前，我们需要回顾正交基的概念。

## 10.1         正交基

定义40.1（正交基）。Rp中的正交基是一组p向量u1，…，up in Rp具有以下特性：

*1.*    u1，…，up是正交的，即hui，uj i：=uTi uj=0，对于i 6=j。

*2.*    每个ui都有单位长度，即每个i的kuik=1。

正交基的最简单例子是e1，…，en，其中ei是第i个位置的向量1和所有其他位置的向量0。

每个正交基u1，…，up满足以下性质：

\1.    u1，…，up是线性独立的，因此构成Rp的基础（这解释了在正交基的定义中存在“基础”一词）想知道为什么这是真的，假设

α1u1+···+αpup=0，对于某些α1，…，αp.（64）

取上述等式两边与uj的点积（对于固定j），我们得到

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image042.gif)

因为只有当i=j和huj，uji=1时，uii才是非零的。因此（64）意味着，对于每个j=1，…，p和u1，…，up，αj=0是线性独立的，因此形成Rp的基础。

\2.    以下公式适用于每个向量x∈Rp：

第页

x=Xhx，uiiui.（65）

i=1

要了解为什么这是真的，首先注意前面的属性意味着u1，…，up构成Rp的基，这样每个x∈Rp都可以写成一个线性组合

x=β1u1+···+βpup

u1的，…，向上现在取两边有uj的点积证明βj=hx，uji。

\3.    公式

（66个）

保持其中Ip是p×p恒等式矩阵要了解为什么这是真的，请注意（65）可以重写为

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image044.gif)

由于上述恒等式的两边对于每个x都是相等的，所以我们必须有（66）。

\4.    假设U是p×p矩阵，其列是向量u1，…，up。那么

UTU=UUT=Ip。

要了解为什么这是真的，请注意UTU的第（i，j）项等于（根据正交基的定义）当i 6=j时为零，否则为1。另一方面，UUT=Ip语句与（66）相同。

\5.    对于每个向量x∈Rp，公式

第页

kxk2=Xhx，uii2

i=1

持有。想知道为什么这是真的，就写

磷

kxk2=xTx=xtutx=kUTxk2=X（uTi X）2=Xhx，uii2。

i=1 i=1

## 10.2         谱定理

定理40.2（谱定理）。假设∑是一个p×p对称矩阵然后存在一个正交基U1，…，UP和实数，1，…，，

（六十七）

谱定理通常也用下面的另一种形式来写。假设U是p×p矩阵，其列是向量u1，…，up还假设∧是p×p对角矩阵（对角矩阵是非对角项都为零的矩阵），其对角项为λ1，…，λp，则（67）等价于

σ=U∧UT和UT∑U=λ。

下面是谱定理的一些直接结果：

\1.    对于每1≤j≤p，我们有

σuj=λjuj和。

这些直接来自（67）。上面的第一个恒等式意味着每个λj是具有特征向量uj的∑的特征值。

\2.    在表示式（67）中，特征值λ1，…，λp是唯一的，而特征向量u1，…，up不一定是唯一的（对于每个uj，可以用-uj替换，如果λ1=λ2，则u1和u2可以由u1和u2跨度内的任意一对正交单位范数向量u1，u∮2替换）。

\3.    σ的秩正好等于非零的λ0j的个数。

\4.    如果所有的λ1，…，λp都是非零的，则∑具有全秩，因此是可逆的而且，我们可以写

.

\5.    如果∑是半正定的，那么（67）中的每个λj都是非负的（这是

0）。

\6.    半正定矩阵的平方根：如果∑是半正定的，那么我们可以定义一个新的矩阵

.

很容易看出∑1/2是对称的，半正定的，满足∑1/2（∑1/2）=∑。我们将∑1/2称为∑的平方根。

\7.    如果∑是正定的，那么（67）中的每个λj都是严格正的（这是

0）。

## 10.3         谱定理的三个应用

40.3.1每个对称半正定矩阵都是协方差矩阵

我们已经看到每个随机向量Y的协方差矩阵Cov（Y）是对称的和半正定的。结果表明，这句话的逆也成立，即对于某个随机向量Y，每一个对称半正定矩阵等于Cov（Y）。为了解释这一点，假设∑是任意p×p对称半正定矩阵。回想一下，通过谱定理，我们定义了∑1/2（∑的平方根），这是一个对称的非负定矩阵，因此∑1/2∑1/2=是∑。

现在假设Z1，…Zp是不相关的随机变量，都有单位方差，让Z=（Z1，…，Zp）T是相应的随机向量因为Z1，…，Zp是不相关的并且具有单位方差，所以很容易看出Cov（Z）=Ip。假设Y=1/2Z，那么

Cov（Y）=Cov（1/2Z）=∑1/2Cov（Z）（1/2）T=1/2（In）∑1/2=σ。

因此，我们从一个任意的半正定矩阵∑开始，证明了它等于某个随机向量Y的Cov（Y）。

因此，我们可以总结协方差矩阵的以下性质。

\1.    每个随机向量的协方差矩阵都是半正定的。

\2.    每一个半正定矩阵等于某个随机向量的协方差矩阵。

\3.    除非随机变量Y1，…，Yn满足一个精确的线性方程，否则它们的协方差矩阵是正定的。

40.3.2美白

给定一个p×1随机向量Y，我们如何将其转换为p×1白噪声向量Z（回想一下，Z是白噪声意味着EZ=0，Cov（Z）=Ip）这种转变被称为美白。如果Cov（Y）为正定，则可以进行美白。确实，假设∑：=Cov（Y）是正定的，具有谱表示：

.

是正定的这一事实意味着，对于每个i=1，…，p，λi>0。在这种情况下，很容易看出

1/2是可逆的

.

此外，很容易检查∑-1/2∑∑-1/2=Ip。使用这个，很容易检查Z=

是白噪声。实际上EZ=0和

.

因此，谱定理被用来定义矩阵（Cov（Y））-1/2，该矩阵可用于白化给定的随机向量Y。

40.3.3随机向量的第一主成分

设Y为p×1向量我们认为单位向量a∈Rp（单位向量是范数等于1的向量）是Y-if的第一主成分

每单位向量b的var（aTY）≥var（bTY）。

换言之，单位向量A在所有单位向量B上最大化BTY的方差。

假设∑：=Cov（Y）具有谱表示（67）假设在不丧失一般性的情况下，（67）中出现的特征值λ1，…，λp按降序排列，即。，

λ1≥λ2≥····≥λp≥0。

然后证明向量u1是Y的第一主分量。要看到这一点，只需注意

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image046.gif)

对于每个单位向量b，

.

因此u1是Y的第一主成分注意，第一主成分不是唯一的确实，√

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image047.gif)

-u1也是第一主成分，如果λ1=λ2，则u2和（u1+u2）/2也是第一主成分。