# 1           随机变量、期望和方差

随机变量是一个函数，它将一个数字附加到样本空间的每个元素上。换句话说，它是一个将样本空间映射为实数的函数。

例如，在掷硬币50次的机会实验中，人头的数目是一个随机变量另一个随机变量是第一条尾巴前面的头数另一个随机变量是模式hththt出现的次数。

许多现实生活中的数据，例如（a）伯克利明天的平均气温，（b）这个房间里随机挑选的学生的身高，（c）我明天将接到的电话数量，（d）赫斯特大街9月份发生的事故数量，等等，都可以作为随机变量来处理。

对于每个事件A（回想一下，事件是样本空间的子集），可以关联一个随机变量，如果A发生，则取1，如果A不发生，则取0这称为对应于事件A的指示符随机变量，并由I（A）表示。

随机变量的分布，非正式地说，是对随机变量所取的一组值及其概率的描述。

如果一个随机变量X取一个有限或可数的可能值的不确定集（在这种情况下，我们说X是一个离散随机变量），它的分布由值a1，a2，。。。它需要

| 连同概率说明： |                     |
| -------------- | ------------------- |
| P{X=ai}        | 对于i=1,2，。。。。 |

将ai映射到P{X=ai}的函数称为离散随机变量X的概率质量函数（pmf）。

如果一个随机变量X取一组连续的值，它的分布通常由一个称为概率密度函数（pdf）的函数来描述pdf是R上的一个函数f，它满足f（x）≥0对每个x∈R和

.

随机变量的pdf f可用于计算每个集合a通孔的P{X∈a}

Z轴

P{X∈A}=f（X）dx。

一个

注意，如果X有密度f，那么对于每个y∈R，

.

重要的是要记住，随机变量的密度函数f（x）并不代表概率（特别是，f（x）取的值远大于1是很常见的）相反，f（x）的值可以被认为是一个比例常数这是因为通常（只要f在x处是连续的）：

.

随机变量X的累积分布函数（cdf）定义为

F（x）：=P{x≤x}，对于－∞<x＜∞。

这是为所有离散或连续的随机变量定义的如果随机变量X具有密度，则其cdf由

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.gif)

cdf具有以下性质：（a）它是非递减的，（b）右连续的，（c）limx ~－－∞F（x）=0和limx ~＋∞F（x）=1。

## 1.1         随机变量的期望

设X为离散随机变量，g为X范围内的实值函数，则g（X）具有有限期望

十

|g（x）| P{x=x}<∞

十

其中总和在x的所有可能值x上。注意g（x）上的绝对值。

当g（X）有有限期望时，我们定义Eg（X）为

（三）

在这里，求和是在所有可能的x的x上。

类似地，如果X是具有密度（pdf）f的连续随机变量，那么我们说g（X）具有有限期望

.

当g（X）有有限期望时，我们定义Eg（X）为

（四）

为什么在定义期望值之前，我们需要确保绝对和（或积分）的有限性？因为否则（3）中的和（或（4）中的积分）可能定义不清例如，当X是离散随机变量时，其值为…，-3，-2，-1,1,2,3有概率的

对于i∈Z，i 6=0然后g（x）=x的（3）和变成

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.gif)

这是毫无意义的这里很容易看出P x | x | P{x=x}=∞。

如果A是一个事件，那么回想一下I（A）表示对应的指示符随机变量，当A保持时等于1，当A不保持时等于0。很容易注意到I（A）的期望值正好等于P（A）。

需要记住的一点是期望是一个线性算子，即。，

E（aX+bY）=aE（X）+bE（Y）

对于任意两个期望值有限的随机变量X和Y以及实数a和b。

## 1.2         方差

如果X2有有限期望，则随机变量X的方差是有限的（你知道当X2有有限期望时，X也会有有限期望吗你将如何证明这一点？）。在这种情况下，X2的方差定义为

V ar（X）：=E（X––X）2=E（X2）–––2X，其中μX：=E（X）。

从定义中可以清楚地看出，随机变量X的方差衡量X在其均值E（X）周围所取值的平均变异性。

假设X是一个离散随机变量，具有有限多个概率值x1，…，xn。那么X的方差是多少？

X方差的平方根称为X的标准差，通常用SD（X）表示。

随机变量X的期望具有以下变分性质：是a的值使所有实数a上的数量E（X-a）2最小化。你知道如何证明这一点吗？

如果随机变量X的方差很小，则X不能偏离其平均值（=E（X）=μ）。这可以通过Chebyshev不等式来精确说明，它说明了以下几点。

Chebyshev不等式：设X为方差有限且均值为μ的随机变量然后，对于每一个>0，以下不等式成立：

.

换言之，X偏离其平均值以上的概率由

.

切比雪夫不等式的证明

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image006.gif)

同时考虑两边的期望值（在左边，我们有一个指标随机变量，它取值1 when，否则取0）。

# 2           随机变量的独立性

我们说两个随机变量X和Y是独立的，如果对任何涉及Y的事件的条件作用不改变任何涉及X的事件的概率，即。，

P{X∈A | Y∈B}=P{X∈A}

对每个A和B。

同样地，X和Y的独立性与

P{X∈A，Y∈B}=P{X∈A}P{Y∈B}

对每个A和B。

以下是独立的后果。如果X和Y是独立的，那么

\1.    对于每对函数g和h，g（X）和h（Y）是独立的。

\2.    E（g（X）h（Y））=每对函数g和h的Eg（X）Eh（Y）。

更一般地，我们说随机变量X1，…，Xk是（相互）独立的，如果对于每1≤i≤k，任何涉及Xj的事件的条件作用，j 6=i不改变任何涉及Xi的事件的概率。

从这里可以很容易地导出独立性的性质，例如

P{X1∈A1，…，Xk yn Ak}=P{X1 yn A1}P{X2 yn A2}…P{Xk yn Ak}

对于事件A1，…，Ak的所有可能选择。

# 3           共同分配

## 3.1         Ber（p）分布

如果随机变量X取p{X=1}=p的两个值0和1，则称其具有Ber（p）（带参数p的Bernoulli）分布。

然后注意EX=p和V ar（X）=p（1-p）。p的哪个值是X的最大变量？最小变量？

## 3.2         仓（n，p）分布

随机变量X具有参数n和p的二项分布（n是正整数，p∈[0,1]），如果它取0,1，…，n的值，pmf由

每k=0,1，…，n。

这是二项式系数：

.

一个Bin（n，p）随机变量的主要例子是一枚硬币在n次独立投掷中获得的人头数，人头的概率等于p。

这里有一个关于Mosteller书中二项分布的有趣问题（你可以很容易地在R中计算出这些概率）。

例3.1（摘自Mosteller的书（问题19：Issac Newton帮助Samuel Pepys））。佩皮斯写信给牛顿，问三个事件中哪一个更有可能：一个人（a）掷6个骰子时至少得到16个，（b）掷12个骰子时至少得到2个6，或（c）掷18个骰子时至少得到3个6，答案是什么？

设X表示一枚硬币n次独立抛掷中的头部数，头部概率为p，那么我们知道X∼Bin（n，p）。如果，现在，Xi表示二元随机变量，如果第i次投掷是头部，则取1，如果第i次投掷是尾部，则取0，那么应该清楚

X=X1+····+Xn。

注意，每个Xi是Ber（p）随机变量，X1，…，Xn是独立的。因此，Bin（n，p）随机变量可以看作n个独立Ber（p）随机变量的和中心极限定理（我们将在后面的课程中详细研究）意味着大量i.i.d的和（i.i.d是什么？）随机变量近似正常。这意味着当n大且p固定时，Bin（n，p）分布看起来像正态分布。我们稍后会把这一点说清楚的。特别地，这意味着二项式概率可以通过N个大P和P固定的正常概率近似计算。从这个角度来看，当k较大时，从6k卷的模具中得到k个或更多个六角的概率是多少？

Bin（n，p）分布的平均值是多少？根据一枚硬币的n次独立投掷，对头部概率的无偏估计是什么？Bin（n，p）的方差是多少？

## 3.3         泊松分布

如果X取0,1,2，…，则随机变量X具有参数λ>0的泊松分布（用Poi（λ）表示）pmf由

对于k=0,1,2，。。。。

泊松分布的主要效用来自以下事实：

事实：二项式分布Bin（n，p）很好地近似于泊松分布POI（NP），只要数量NP2小。

要直观地看到为什么这是真的，只要看看

P{Bin（n，P）=0}=（1P）n=exp（nlog（1P））。

现在注意，np2很小意味着p很小（注意p可以写成pnp2/n≤pnp2，所以小np2必然意味着p很小）当p为小时，我们可以将log（1～p）近似为p p2／2，从而得到

.

现在因为NP2小，所以我们可以忽略上面的第二个术语，得到p{bin（n，p）＝0 }近似由Ep（NP）精确地等于p{POI（NP）＝0 }。对于每一个固定k＝0，1，2，……，同样可以用p{POI（NP）＝k}近似地逼近p{ Bin（n，p）＝k}。

有一个形式定理（称为Le-Cam定理）严格证明了当np2很小时Bin（n，p）≈Poi（np）以下无需证明（其证明不在本类范围内）。

定理3.2（勒卡姆定理）假设X1，…，X n是独立的随机变量，因此对于某些pi∈[0,1]对于i=1，…，n，Xi∼Ber（pi），设X=X1+····+Xn和λ=p1+…pn那么

.

在特殊情况下，当p1=···=pn=p时，上述定理表明

∞

X | P{Bin（n，P）=k}-P{Poi（np）=k}|<2np2

k=0

因此，当np2很小时，对于每个k=0,1，…，概率P{Bin（n，P）=k}接近于P{Poi（np）=k}这个事实的一个含义是，对于每个固定的λ>0，我们有

当n很大时。

这是因为当p=λ/n时，我们有np2=λ2/n，当n较大时，它会很小。

泊松分布的这种近似性质是泊松分布用于模拟稀有事件计数的原因。例如，一个电话接线员一天接到的电话号码、一天中某条街道上发生的事故数目、一本书中发现的打字错误数目、一场足球比赛中进球的数目，都可以被模拟成Poi（λ）的某个λ>0。你能解释为什么这些现实生活中的随机量可以用泊松分布来建模吗？

下面的例子给出了泊松分布提供良好近似的另一种情况。

例3.3。假设n个字母编号为1，…，n和n个信封编号为1，…，n。字母i的右信封是信封i。假设我取随机排列σ1，…，σn为1，…，n，然后将字母σi放入信封i中。让X表示在其右信封中的字母数X的分布是什么？

设Xi为随机变量，当第i个字母在第i个信封中时取1，否则取0那么很明显X=X1+····+Xn。注意

对于每个i=1，…，n。

这是因为第i封信同样可能在n个信封中的任何一个信封中。因此，这意味着

当i=1，…，n时，Xi∼Ber（1/n）。

如果X i也是独立的，那么X=X1+····+X n将是Bin（n，1/n），这对于大n非常接近Poi（1）。但是Xi在这里不是独立的，因为对于i 6=j，

.

但依赖性相对较弱，X的分布与Poi（1）非常接近。我们将通过证明p{x＝0 }近似等于p{POI（1）＝0 }＝E＝1来证明这一点。我将作为一个练习来展示，对于每个固定的k，P{X=k}≈P{Poi（1）=k}。要计算P{X=0}，我们可以编写

.

现在请注意，对于每个i1

.

这给了

.

很容易证明Poi（λ）随机变量的期望和方差都等于λ。这也很有意义，因为两者之间的联系：

Poi（λ）≈Bin（n，λ/n）

作为

E（Bin（n，λ/n））=λ和。

当通过泊松分布对计数数据建模时，可以通过经验检验方差等于平均值的假设。如果经验方差似乎远高于平均值，那么有人说存在过度分散，在这种情况下，泊松可能不是一个好的数据模型。

# 4           协方差、相关与回归

给定两个随机变量X和Y（使得X2和Y 2具有有限期望），X和Y之间的协方差用Cov（X，Y）表示，并定义为

Cov（X，Y）：=E[（X-μX）（Y-μY）]（5）

式中，μX：=E（X）和μY：=E（Y）。换句话说，Cov（X，Y）被定义为随机变量（X-？X）（Y-？Y）的期望值（但是这个随机变量有有限的期望值吗？你能证明这是假设X2和Y 2有有限期望的结果吗？）.

重要的是要注意协方差是双线性算子，即。，

Cov（XaiXi，XbjYj）=XXaibjCov（Xi，Yj）。（6）

我会的

你能证明这是协方差定义（5）和期望算子线性的结果吗？

当X=Y时，很容易看出Cov（X，X）只是X的方差。利用协方差与方差和（6）之间的关系，可以推导出方差的下列标准性质：

\1.    V ar（aX+b）=a2V ar（X）。

\2.    V ar（Pi aiXi）=Pi a2iV ar（Xi）+Pi6=j aiajCov（Xi，Xj）。

两个随机变量X和Y（使得X2和Y 2具有有限方差）之间的相关性定义为：

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image008.gif)

如果ρX，Y=0，我们说X和Y是不相关的。

提议4.1关于相关性的两个事实：

*1.*    相关系数ρX，Y总是介于-1和1之间。

*2.*    对于每一个a，b，c，d∈（－∞，∞）。换句话说，在线性变换下，相关性是不变的（直至符号翻转）。

证明写

.

使用标准不等式：

（七）

用a=（X-μX）/pV ar（X）和b=（Y-μY）/pV ar（Y）获得

.

这证明了ρX，Y≤1为了证明ρX，Y<-1，用

（八）

关于相关性和线性函数的事实作为练习留在这里。

Cauchy-Schwartz不等式：相关性ρX，Y介于-1和1之间的事实有时通过Cauchy-Schwartz不等式得到证明，该不等式指出：对于每对随机变量Z1和Z2，我们有

)（九）

根据上述不等式，取Z1=X-μX和Z2=Y-μY，即可推导出|ρX，Y |≤1的事实。

你能用（7）和（8）证明柯西-施瓦兹不等式（9）吗？

不相关性与独立性：下面总结了不相关性与独立性的关系：

\1.    两个独立的随机变量X和Y是不相关的。

\2.    存在许多不相关的随机变量X和Y的非独立的例子。你能想出几个吗？

\3.    当且仅当g（X）和h（Y）对于每对函数g和h不相关时，两个随机变量X和Y是独立的。

# 5           相关与回归

ρX，Y的一个重要性质是它测量X和Y之间的线性关联强度。本节对此进行了说明。考虑由x的线性函数β0＋β1x逼近随机变量y的问题。对于给定数β0和β1，让我们用均方误差测量β0 +β1x近似y的精度：

L（β0，β1）：=E（Y-β0-β1X）2。

如果β0＋β1x是一个很好的近似y，则L（β0，β1）应该是低的。相反，如果β0＋β1X是Y的差近似，则L（β0，β1）应该是高的。当β0和β1在所有实数上变化时，L（β0，β1）的最小可能值是多少。

可以看出

（十）

你知道如何证明以上的情况吗？

事实（10）精确地捕获了相关测量Y和X之间的线性关联强度的解释。这是因为Minβ0、β1 L（β0、β1）代表了Y和X（10）的线性组合近似Y的最小可能均方误差，这表明它与Y和X.之间的相关性直接相关。

你能明确地写下使L（β0，β1）最小的β0和β1的值吗？

以上有没有让你想起线性回归？以什么方式？

# 6           回到共同分配

在上一节课中，我们研究了二项分布和泊松分布。这两种分布都是离散分布，可由抛硬币引起（Bin（n，p）是硬币在n次独立抛硬币中的头部数分布，头部概率p和Poi（λ）≈Bin（n，λ/n））我们现在将修正另外两个由抛硬币产生的离散分布：几何分布和负二项分布。

## 6.1         几何分布

如果X取1,2，…，则X具有参数p∈[0,1]的几何分布（写成X∼Geo（p））概率：

P{X=k}=（1-P）k-1p对于k=1,2，。。。。

很容易看出，获得第一个头部所需的独立投掷次数（硬币的头部概率为p）具有Geo（p）分布。

Geo（p）分布具有无记忆性的有趣性质，即如果X∼Geo（p），那么

P{X>m+n | X>n}=P{X>m}。（11）

当P{X>m}=（1-P）m时，这很容易检查。有趣的是，几何分布是{1,2，…}上唯一满足无记忆特性（11）的分布为此，假设X是一个满足（11）的随机变量，它接受{1,2，…}中的值设G（m）：=P{X>m}，m=1,2，。。。。那么（11）与

G（m+n）=G（m）G（n）。

这清楚地给出了G（m）=（G（1））m，每m=1,2现在G（1）=P{X>1}=1-P{X=1}如果p=1-p{X=1}，则

G（m）=（1–（p）m

这意味着P{X=k}=P{X>k-1}-P{X>k}=P（1-P）k-1对于每k≥1意味着X是

地理位置（p）。

## 6.2         负二项分布

设X表示获得第k个头所需的投掷次数（硬币的头部概率为p）。下面给出X的分布。X取k，k+1，。。。和

这称为负二项分布，参数为k和p（用NB（k，p）表示）如果G1，…，Gk是独立的Geo（p）随机变量，那么G1+···+Gk∼NB（k，p）（你能证明吗？）.

# 7           连续分布

## 7.1         正态或高斯分布

随机变量X具有正态分布，平均值μ，方差σ2>0，如果它具有以下pdf：

.

我们写X∼N（μ，σ2）。当μ=0和σ2=1时，我们认为X具有标准正态分布，标准正态pdf简单地用φ（·）表示：

.

2√

你知道为什么φ（·）是一个有效的密度，也就是为什么R e-x/2dx=2π吗？

标准正态cdf用Φ（x）表示：

Z xΦ（x）：=φ（t）dt。

−∞

如果X∼N（μ，σ2），则E（X）=μ，V ar（X）=σ2有关正态分布的许多属性的列表，请参见相应的维基百科页面中心极限定理是正态分布是统计学中最突出的分布的主要原因。

## 7.2         均匀分布

如果随机变量U具有以下pdf，则称其在（0,1）上具有均匀分布：

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.gif)

我们写U∼U[0,1]你是什么意思你的方差是多少？在统计学中，统一分布在哪里空分布下的p值通常根据U[0,1]分布进行分布（稍后详细介绍）。

更一般地，给定一个区间（a，b），我们说随机变量U在（a，b）上具有均匀分布，如果它具有以下pdf：

：a<x<b

0:对于所有其他x

我们把它写成U∼U（a，b）。

## 7.3         指数密度

速率参数λ>0的指数密度（用Exp（λ）表示）由下式给出

f（x）=λe--λxI{x>0}。

它可以说是建模被限制为非负的随机量的最简单密度。它用来模拟话务员从现在开始接到的第一个电话的时间一般来说，它是随着Poisson过程中到达间隔时间的分布而产生的（稍后我们研究Poisson过程时将对此进行详细说明）。Exp（λ）的cdf很容易被认为是

对于x>0。

指数密度具有无记忆性（就像几何分布一样）。的确，

.

事实上，指数密度是（0，~）上唯一具有无记忆性的密度（证明留作练习）。在这个意义上，指数分布x可以看作是几何分布的连续模拟。

## 7.4         伽马密度

通常在指数密度之后讨论伽马密度形状参数α>0、速率参数λ>0的伽马密度由下式给出

f（x）107xα-1e-λxI{x>0}（12）

为了找到上面的比例常数，我们需要评估

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image012.gif)

现在函数

对于α>0

在数学上叫做伽玛函数所以（12）中的比例常数由

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image014.gif)

所以伽马密度的公式是：

.

我们称之为伽马（α，λ）密度。

注意，当α=1时，伽马（α，λ）密度减小到Exp（λ）密度因此，伽马密度可以看作是指数密度的推广。事实上，伽马密度可以看作是负二项分布的连续模拟，因为如果X1，…，Xk是独立的Exp（λ）随机变量，那么X1+···+Xn∼伽马（k，λ）（因此伽马分布作为i.i.d指数之和出现，正如负二项分布作为i.i.d几何随机变量之和出现一样）。

下面是伽马函数的一些基本性质，对我们以后的工作有帮助对于任意α>0，Gamma函数没有闭合表达式但是当α是正整数时，

| 可以看出                                    |         |          |
| ------------------------------------------- | ------- | -------- |
| Γ（n）=（n-1）！   上述不平等是该性质的结果 | n≥1时。 | （十三） |
| Γ（α+1）=αΓ（α）                            | 对于α>0 | （十四） |

以及Γ（1）=1这一微不足道的事实你可以很容易地通过部分集成来验证（14）。

√

关于Gamma函数的另一个简单事实是Γ（1/2）=π（这是

2√

R e-x/2dx=2π）。

# 8           变量转换

通常采用随机变量的函数或变换。考虑一个随机变量X，对X应用一个函数u（·）将X转换成另一个随机变量Y=u（X）。如何从X的分布中找到Y=u（X）的分布？

如果X是离散随机变量，那么Y=u（X）也将是离散的，那么Y的pmf可以直接用X的pmf来表示：

P{Y=Y}=P{u（X）=Y}=X P{X=X}。

x:u（x）=y

如果X是一个密度为f的连续随机变量，u（·）是一个光滑函数，那么用f来记下Y=u（X）的密度是相当简单的。有一些通用的公式可以这样做，但最好从第一性原理开始学习。我将用以下两个例子来说明这个总体思路。

例8.1。假设X∼U（π/2，π/2）。Y=tan（X）的密度是多少这是从第一原理开始的方法注意tan（x）与x的距离在（－∏/2，π/2）上的范围是R，所以固定y∈R，我们会发现y在y的密度g以下。

g（y）的公式是

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image016.gif)

以便

当δ较小时，P{y<y<y+δ}≈g（y）δ。（15）

对于小δ，

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image018.gif)

其中f是X的密度，与（15）相比，我们可以得出结论

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image020.gif)

利用X∼U（－∏/2，π/2）的密度，我们推断

对于y∈R。

这是柯西密度。你能把上述论点严格化吗如果教你计算变换密度的公式，这个公式和这里的答案匹配吗？

例8.2。假设X∼N（0,1），使X具有标准法向密度φ（·）。Y=X2的密度是多少？下面的方法是从第一原则开始的。X2的范围是X的范围在（－～～，～）以上，所以我们把y定为0。我们将在Y处找到Y的密度g

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image022.gif)

这给了

当y>0时。

这是具有1个自由度的卡方随机变量（或形状参数α=1/2和比例参数β=1/2的伽马随机变量）的密度。

# 9           分布函数与分位数变换

随机变量X的分布函数（cdf）定义为

F（x）：=P{x≤x}，对于－∞<x＜∞。

cdf具有以下属性：

\1.    它是非递减的，即当x≤y时，F（x）≤F（y）。

\2.    它接受介于0和1之间的值。三。每一个x，我们都有

还有林。

这特别意味着F是右连续的，F的连续性相当于每X的P{X=X}=0。

四。函数F（x）接近0作为x······，接近1作为x·········。

上述性质刻画了cdf，即满足这四个性质的（－～～，∞）上的每个函数F等于某个随机变量的cdf证明这一点的一种方法是通过分位数变换。

给定满足上述四个性质的函数F，相应的分位数变换（或分位数函数）qF是（0,1）上的实值函数，定义为

qF（u）：=inf{x∈R:F（x）≥u}。（16）

分位数变换可视为cdf F的某种逆变换。事实上，当cdf F连续且严格增加时，分位数函数qF正好等于F−1。

分位数变换的基本性质如下对于x∈R和0<u<1：

当且仅当x≥qF（u）时F（x）≥u（17）

这是（17）的证据。当F（x）≥u时，从qF的定义可以明显看出x≥qF（u）。

另一方面，根据qF（u）的定义和F不递减的事实，我们得到了每大于0。设0，利用F的右连续性，我们推导出F（qF（u））≥u。这意味着当x≥qF（u）时，我们有F（x）≥F（qF（u））≥u。这证明了（17）。

分位数变换的一个重要事实是：

P{X≤qF（u）}≥u和P{X<qF（u）}≤u。（18）

因此，qF（u）是X的分位数（当u=0.5时，则qF（0.5）是X的中位数）因此命名为分位数变换。上面的第一个不等式是

P{X≤qF（u）}=F（qF（u））≥u。

要证明第二个不等式，请注意，根据qF（u）的定义，

为每一个人。

设0，我们得到了证明（18）的P{X<qF（u）}≤u。

下面的结果是分位数变换的一个重要应用。

提议9.1以下两种说法是正确的。

*1.*    假设U是根据（0,1）上的均匀分布而分布的随机变量然后qF（U）有cdf。

*2.*    假设X是具有连续cdf F的随机变量，则F（X）在（0,1）上具有均匀分布。

证明对于第一部分，首先注意均匀随机变量U的cdf FU满足FU（U）=U

0≤u≤1因此，X=qF（U）的cdf FX由

F x（x）=P{x≤x}=P{qF（U）≤x}=P{U≤F（x）}=F（x）

我们最常用的（17）这证明X有cdf。

对于第二部分，假设F是连续的。注意，对于每一个>0，qF的定义意味着。设0，利用F的连续性，我们推导出F（qF（u））≤u，并与（17）结合，得到F（qF（u））=u，因此对于每0<u<1，我们有

P{F（X）≥u}=P{X≥qF（u）}=1-P{X<qF（u）}=1-P{X≤qF（u）}=1-F（qF（u））=1-u

其中我们使用了F的连续性（这意味着P{X=X}对于每个X）每0<u<1，P{F（X）≥u}=1-u这一事实意味着F（X）∼u（0,1）。

例9.2（在零假设下，与具有连续分布的检验统计量相对应的p值具有均匀分布）。统计假设检验问题通常是根据数据计算相关的检验统计量而形成的。假设Tobs是根据数据计算的统计值的观测值。与检验相对应的p值被定义为在零假设下观察统计值的概率，该统计值比Tobs更极端。通常计算为

p=1-F0（托布斯）

其中F0是零假设下检验统计量的cdf如果F0是一个连续的cdf，那么当Tobs∼F0时，应该清楚p是根据U（0,1）分布的换句话说，在零分布（即Tobs∼F0）下，p值具有标准均匀分布。

# 10          接缝密度

节点密度用于描述有限组连续随机变量的分布我们关注双变量关节密度（即当存在两个连续变量X和Y时）。对于两个以上的变量，这两种观点是相同的。

以下是关于关节密度要记住的要点：1f（·，·）称为接头密度，如果

f（x，y）≥0，对于所有x，y和。

\2.    我们说两个随机变量X和Y有联合密度f（·，·）如果

Z Z Z Z

P{（X，Y）∈B}=f（X，Y）dxdy=I{（X，Y）∈B}f（X，Y）dxdy。

乙

对于R2的每一个子集B我们通常用fX，Y来表示（X，Y）的节理密度。

\3.    如果∏是R2中围绕一个点（x0，yo）的一个小区域，我们有（在某些规律性条件下，fX，Y在（x0，y0）处的行为）

P{（X，Y）∈∏}≈（面积∏）fX，Y（x0，y0）。

更正式地说，

P{（X，Y）∈{X

其中，极限值取∏收缩至（x0，y0）。

\4.    如果（X，Y）具有关节密度fX，Y，则X的密度由fX给出，Y的密度为fY，其中

Z Z轴

fX（x）=fX，Y（x，Y）dy和fY（Y）=fX，Y（x，Y）dx。

密度fX和fY分别称为X和Y的边缘密度。

\5.    独立性和接头密度：以下陈述是等效的：

(a)    随机变量X和Y是独立的。

(b)    关节密度fX，Y（x）分解为一个单独依赖于x的函数和一个单独依赖于Y的函数的乘积。

(c)    fX，Y（x，Y）=所有x，Y的fX（x）fY（Y）。

例10.1考虑功能

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image024.gif)

检查这确实是一个密度函数这个密度取单位平方上的值1如果随机变量X和Y有这个密度f，那么我们说它们在单位平方上是均匀分布的。使用指示符函数，我们还可以将此密度写为：

f（x，y）=I{0≤x≤1，0≤y≤1}=I{0≤x≤1}I{0≤y≤1}

上面的因式分解立即表明，如果f=f X，Y，那么X和Y是独立的。X和Y的边缘密度是[0,1]上的均匀密度。

问题：如果X，Y有这个密度f，计算P{X2+Y 2≤1}（Ans:π/4）。

例10.2假设X，Y有关节密度

.

表明X的边缘密度由

.

X和Y是独立的吗（回答：不，为什么？）